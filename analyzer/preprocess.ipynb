{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "posts = pd.read_csv(\"../data/Suicide_Detection.csv\")\n",
    "posts.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "posts[[\"class\"]] = (posts[[\"class\"]] == \"suicide\").astype(\"int16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(posts, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = strat_train_set.copy()\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_lengths = [len(post.split()) for post in posts[\"text\"] if len(post.split()) < 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(post_lengths, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set, strat_val_set = train_test_split(posts, test_size=1/9, random_state=1)\n",
    "\n",
    "strat_train_set[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "spacy.require_gpu()\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('I was reading the paper.')\n",
    "print([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# from alive_progress import alive_it\n",
    "\n",
    "def preprocess_set(set, directory):\n",
    "    print(f'Preprocessing {directory} data')\n",
    "\n",
    "    texts = set.copy()['text']\n",
    "    labels = set.copy()['class']\n",
    "    texts = [' '.join(text.split()[:500]) for text in texts]\n",
    "\n",
    "    docs = (doc for doc in (nlp.pipe(texts)))\n",
    "    processed_texts = []\n",
    "\n",
    "    for doc in docs:\n",
    "        lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        processed_texts.append(' '.join(lemmas))\n",
    "\n",
    "    # for doc in alive_it(docs, total=len(texts)):\n",
    "    #     lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    #     processed_texts.append(' '.join(lemmas))\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "\n",
    "    from pathlib import Path\n",
    "    Path(f\"{directory}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(f\"{directory}/texts.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(processed_texts, fp)\n",
    "    \n",
    "    with open(f\"{directory}/labels.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(labels, fp)\n",
    "\n",
    "    return processed_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts, labels = preprocess_set(strat_train_set, 'train')\n",
    "preprocess_set(strat_val_set, 'val')\n",
    "preprocess_set(strat_test_set, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
